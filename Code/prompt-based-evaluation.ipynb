{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers accelerate huggingface_hub\n",
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:01:34.894735Z",
     "iopub.status.busy": "2025-01-31T02:01:34.894416Z",
     "iopub.status.idle": "2025-01-31T02:01:54.251503Z",
     "shell.execute_reply": "2025-01-31T02:01:54.250533Z",
     "shell.execute_reply.started": "2025-01-31T02:01:34.894697Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "from torch import cuda, bfloat16\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "#from groq import Groq\n",
    "#import openai\n",
    "from google.api_core.exceptions import InternalServerError\n",
    "import re\n",
    "from huggingface_hub import login\n",
    "from transformers import (AutoModelForCausalLM, AutoModel,\n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "login(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:01:54.253953Z",
     "iopub.status.busy": "2025-01-31T02:01:54.253457Z",
     "iopub.status.idle": "2025-01-31T02:01:55.328502Z",
     "shell.execute_reply": "2025-01-31T02:01:55.327550Z",
     "shell.execute_reply.started": "2025-01-31T02:01:54.253922Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "#from peft import LoraConfig, PeftConfig\n",
    "# from trl import SFTTrainer\n",
    "# from trl import setup_chat_format\n",
    "from transformers import (AutoModelForCausalLM, AutoModel,\n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, precision_recall_curve\n",
    "from sklearn.isotonic import IsotonicRegression       #For calibrating the probs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "\n",
    "\n",
    "# There are important!\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:01:55.330494Z",
     "iopub.status.busy": "2025-01-31T02:01:55.329758Z",
     "iopub.status.idle": "2025-01-31T02:01:55.334729Z",
     "shell.execute_reply": "2025-01-31T02:01:55.333845Z",
     "shell.execute_reply.started": "2025-01-31T02:01:55.330465Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:01:55.363457Z",
     "iopub.status.busy": "2025-01-31T02:01:55.363230Z",
     "iopub.status.idle": "2025-01-31T02:01:55.373457Z",
     "shell.execute_reply": "2025-01-31T02:01:55.372666Z",
     "shell.execute_reply.started": "2025-01-31T02:01:55.363434Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,  #False\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\" #\"float16\", torch.float16,\n",
    "    #load_in_8bit_fp32_cpu_offload=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:01:55.374744Z",
     "iopub.status.busy": "2025-01-31T02:01:55.374458Z",
     "iopub.status.idle": "2025-01-31T02:01:55.386846Z",
     "shell.execute_reply": "2025-01-31T02:01:55.386098Z",
     "shell.execute_reply.started": "2025-01-31T02:01:55.374712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize an empty list to store the embeddings\n",
    "ArticleEmbedding = []\n",
    "\n",
    "def compute_article_embedding(embed, tokenizer):\n",
    "    # Load the dataset from the JSON file\n",
    "    with open(\"/kaggle/input/dataset-multi-class-classification/NewsBlog_ProthomAlo_FinalDataset.json\", \"r\") as f:\n",
    "        data = json.loads(f.read())\n",
    "        \n",
    "        # Set the model to evaluation mode (disables dropout, etc.)\n",
    "        embed.eval()\n",
    "        \n",
    "        # Loop through each article in the dataset\n",
    "        for l, article_data in enumerate(data):\n",
    "            article = dict()\n",
    "            print(f\"Processing article number: {l + 1}\")\n",
    "            \n",
    "            # Get the article text\n",
    "            article_text = article_data['Article Text']\n",
    "            \n",
    "            # Tokenize the article text\n",
    "            inputs = tokenizer(article_text, return_tensors=\"pt\", truncation=True, max_length=256)  # Limit to 512 tokens\n",
    "\n",
    "            \n",
    "            # Perform inference without storing gradients\n",
    "            with torch.no_grad():\n",
    "                embedding = embed(**inputs, output_hidden_states=True)\n",
    "                hidden_state = embedding.hidden_states[-1]\n",
    "            # Take the mean of the embedding along the sequence dimension (dim=1) to get a single vector for the article\n",
    "            article_embedding = hidden_state.mean(dim=1).squeeze()\n",
    "            \n",
    "            # Add the embedding and other details to the article dictionary\n",
    "            article['Article Text'] = article_text\n",
    "            article['Article Embedding'] = article_embedding\n",
    "            article['Label'] = article_data.get('Concept')\n",
    "            \n",
    "            # Append the processed article to the list of embeddings\n",
    "            ArticleEmbedding.append(article)\n",
    "            torch.cuda.empty_cache()\n",
    "    return ArticleEmbedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T02:01:55.389630Z",
     "iopub.status.busy": "2025-01-31T02:01:55.389378Z",
     "iopub.status.idle": "2025-01-31T02:01:55.402249Z",
     "shell.execute_reply": "2025-01-31T02:01:55.401431Z",
     "shell.execute_reply.started": "2025-01-31T02:01:55.389606Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_label_embedding(embed, tokenizer):\n",
    "    print(\"inside label embedding\")\n",
    "    with open(\"/kaggle/input/dataset-multi-class-classification/Keyword_BengaliNews.json\", \"r\") as f:\n",
    "        key = json.loads(f.read())\n",
    "        \n",
    "        for i in range(len(key)):\n",
    "            label_embedding = 0\n",
    "            label = key[i]['Keyword'][0]\n",
    "            \n",
    "            for j in range(len(key[i]['Keyword'])):\n",
    "                # Tokenize the keyword\n",
    "                inputs = tokenizer(key[i]['Keyword'][j], return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "                \n",
    "                # Perform inference without gradients\n",
    "                with torch.no_grad():\n",
    "                    # Forward pass through the model\n",
    "                    outputs = embed(**inputs, output_hidden_states=True)\n",
    "                    \n",
    "                    # Extract the last hidden state tensor\n",
    "                    hidden_state = outputs.hidden_states[-1]  # This gives you the tensor\n",
    "                    \n",
    "                    # Mean pooling across the sequence dimension (dim=1)\n",
    "                    keyword_embedding = hidden_state.mean(dim=1).squeeze()\n",
    "                    \n",
    "                    # Accumulate embeddings\n",
    "                    label_embedding += keyword_embedding\n",
    "            \n",
    "            # Store averaged embeddings\n",
    "            LabelEmbedding[label] = label_embedding / len(key[i]['Keyword'])\n",
    "    \n",
    "    return LabelEmbedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-31T02:23:50.048298Z",
     "iopub.status.idle": "2025-01-31T02:23:50.048589Z",
     "shell.execute_reply": "2025-01-31T02:23:50.048467Z",
     "shell.execute_reply.started": "2025-01-31T02:23:50.048454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_path=\"/kaggle/input/dataset-multi-class-classification/NewsBlog_ProthomAlo_FinalDataset.json\"\n",
    "result_path=\"/kaggle/working/output(1000-2000)\"\n",
    "\n",
    "#torch.cuda.empty_cache()\n",
    "#device_map={\"\": 0}\n",
    "#Load the model here\n",
    "base_model = \"BanglaLLM/bangla-llama-13b-instruct-v0.1\" \n",
    "\n",
    "base_model_name = base_model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"float16\",\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "# model = AutoModel.from_pretrained(base_model_name, device_map=\"auto\", offload_folder=\"offload\")\n",
    "# model.gradient_checkpointing_enable()\n",
    "# Load the tokenizer here if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=True, trust_remote_code=True)\n",
    "\n",
    "#LaBSE implementation\n",
    "# model = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "\n",
    "#LASER implementation\n",
    "# laser = Laser()\n",
    "\n",
    "# Bangla transformer implementation\n",
    "# transformer=Bangla_sentence_transformer_small()\n",
    "\n",
    "\n",
    "#Bloom implementation\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-1b1\")\n",
    "# model = BloomModel.from_pretrained(\"bigscience/bloomz-1b1\")\n",
    "\n",
    "\n",
    "#Flan-UL2 implementation\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-ul2\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\")\n",
    "\n",
    "#GPTNeoX implementation\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "# model = GPTNeoXModel.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "LabelEmbedding={}\n",
    "ArticleEmbedding=[]\n",
    "\n",
    "LabelEmbedding= compute_label_embedding2(model, tokenizer)\n",
    "ArticleEmbedding= compute_article_embedding(model, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-31T02:23:50.049992Z",
     "iopub.status.idle": "2025-01-31T02:23:50.050377Z",
     "shell.execute_reply": "2025-01-31T02:23:50.050227Z",
     "shell.execute_reply.started": "2025-01-31T02:23:50.050210Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for threshold in np.arange(0.95,0.99,0.05):\n",
    "        final_article_list = []\n",
    "        print(\"Threshold running for::\",threshold)\n",
    "        filename = str(threshold) + '_NewsBlog_ProthomAlo_Dataset.json'\n",
    "        for i in range(0, len(ArticleEmbedding)):\n",
    "            inferred_label=[]\n",
    "            final_article = {}\n",
    "            for label in LabelEmbedding:\n",
    "                x = ArticleEmbedding[i]['Article Embedding']\n",
    "                #.hidden_states[-1]\n",
    "                label_embedding_np = LabelEmbedding[label].cpu().numpy().flatten()\n",
    "                squeezed_embedding_np = x.flatten()\n",
    "                squeezed_embedding_np = [np.mean(squeezed_embedding_np)]\n",
    "                label_embedding_np = [np.mean(label_embedding_np)]\n",
    "                # print(label_embedding_np)\n",
    "                # print(squeezed_embedding_np)\n",
    "                cosine_dist = 1 - spatial.distance.cosine(label_embedding_np, squeezed_embedding_np)\n",
    "                #print(cosine_dist)\n",
    "                if (cosine_dist > threshold):\n",
    "                    inferred_label.append(label)\n",
    "        \n",
    "            final_article['Article Text'] = ArticleEmbedding[i]['Article Text']\n",
    "            final_article['Inferred Article Label'] = inferred_label # inferred labels\n",
    "            final_article['Label'] = ArticleEmbedding[i]['Label']  # ground truth labels added for evaluation\n",
    "            final_article_list.append(final_article)\n",
    "\n",
    "        with open(result_path+filename, 'w', encoding='utf-8', newline='\\n') as f:\n",
    "            #print(\"writing json::\",filename)\n",
    "            json.dump(final_article_list, f, ensure_ascii=False, indent=4, separators=(',', ':'))\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5897384,
     "sourceId": 9654282,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5914932,
     "sourceId": 9677686,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
